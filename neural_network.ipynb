{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1kjR0nFZN_9tAqakqoztE2OiG5OYw4DWK",
      "authorship_tag": "ABX9TyOHA9AHr6Pv72kPQX7mO5xa"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoSRct2DJBhS"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUvW1LrsrvOz"
      },
      "source": [
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt4XhLL1JKoa"
      },
      "source": [
        "class NeuralNetwork:\n",
        "  def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
        "    self.input_nodes = input_nodes\n",
        "    self.hidden_nodes = hidden_nodes\n",
        "    self.output_nodes = output_nodes\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "    self.W1 = np.random.randn(self.input_nodes, self.hidden_nodes) / np.sqrt(self.input_nodes) # Xavier initialization\n",
        "    self.b1 = np.random.rand(1, self.hidden_nodes)\n",
        "\n",
        "    self.W2 = np.random.randn(self.hidden_nodes, self.output_nodes) / np.sqrt(self.hidden_nodes) # Xavier initialization\n",
        "    self.b2 = np.random.rand(1, self.output_nodes)\n",
        "\n",
        "    self.Z1 = np.zeros([1, input_nodes])  \n",
        "    self.O1 = np.zeros([1, input_nodes])\n",
        "\n",
        "    self.Z2 = np.zeros([1, self.hidden_nodes])\n",
        "    self.O2 = np.zeros([1, self.hidden_nodes])\n",
        "\n",
        "    self.Z3 = np.zeros([1, self.output_nodes])\n",
        "    self.O3 = np.zeros([1, self.output_nodes])\n",
        "\n",
        "\n",
        "  def feed_forward(self):\n",
        "    delta = 1e-7  # log 무한대 발산 방지\n",
        "\n",
        "    self.Z1 = self.input_data # input 값 그대로\n",
        "    self.O1 = self.input_data # (1,784)\n",
        "\n",
        "    self.Z2 = np.dot(self.O1, self.W1) + self.b1\n",
        "    self.O2 = sigmoid(self.Z2) # (1,100)\n",
        "\n",
        "    self.Z3 = np.dot(self.O2, self.W2) + self.b2\n",
        "    self.O3 = sigmoid(self.Z3) # (1,10)\n",
        "    h = self.O3\n",
        "\n",
        "    self.loss = -np.mean(self.y * np.log(h + delta) + (1 - self.y) * np.log(1 - h + delta))  # log 무한대 발산 방지를 위해 delta 더해주기\n",
        "\n",
        "\n",
        "  def back_propagation(self):\n",
        "    loss_3 = (self.O3 - self.y) * self.O3 * (1 - self.O3)\n",
        "    self.W2 -= np.dot(self.O2.T, loss_3) * self.learning_rate\n",
        "    self.b2 -= loss_3 * self.learning_rate\n",
        "\n",
        "    loss_2 = np.dot(loss_3, self.W2.T) * self.O2 * (1 - self.O2)\n",
        "    self.W1 -= np.dot(self.O1.T, loss_2) * self.learning_rate\n",
        "    self.b1 -= loss_2 * self.learning_rate\n",
        "\n",
        "\n",
        "  def train(self, input_data, target_data):\n",
        "    self.input_data = input_data\n",
        "    self.y = target_data\n",
        "\n",
        "    self.feed_forward()\n",
        "    \n",
        "    self.back_propagation()\n",
        "\n",
        "\n",
        "  def predict(self, input_data_list, target_data_list):\n",
        "    total_num = target_data_list.shape[0]\n",
        "    answer_num = 0\n",
        "\n",
        "    for idx in range(len(target_data_list)):\n",
        "      input_data = input_data_list[idx] / 255.0 * 0.99 + 0.01\n",
        "      self.input_data = np.array(input_data, ndmin=2)\n",
        "\n",
        "      target_value = target_data_list[idx]\n",
        "\n",
        "      self.feed_forward()\n",
        "\n",
        "      predict_value = np.argmax(self.O3)\n",
        "\n",
        "      if predict_value == target_value:\n",
        "        answer_num += 1\n",
        "\n",
        "    print('Accuracy: ', answer_num / total_num)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntFS4411bYij"
      },
      "source": [
        "input_nodes = 784\n",
        "hidden_nodes = 100\n",
        "output_nodes = 10\n",
        "learning_rate = 0.3\n",
        "\n",
        "# 신경망 객체 생성\n",
        "model = NeuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxRK3o6ujb4j",
        "outputId": "a8d1c273-4b38-4105-fe2e-e5da44ef52be"
      },
      "source": [
        "# Train\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "train_data_list = np.loadtxt('./mnist_train.csv', delimiter=',', dtype=np.float32)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for step, train_data in enumerate(train_data_list):\n",
        "    input_data = train_data[1:] / 255.0 * 0.99 + 0.01  # 0이면 가중치 업데이트가 중지되므로 0.01~1로 만듦\n",
        "\n",
        "    target_data = np.zeros(output_nodes) + 0.01\n",
        "    target_data[int(train_data[0])] = 0.99\n",
        "\n",
        "    model.train(np.array(input_data, ndmin=2), np.array(target_data, ndmin=2))\n",
        "\n",
        "    if step % 1000 == 0:\n",
        "      print('Epoch: {:2d}  Step: {:7d}  Loss: {:10f}'.format(epoch, step, model.loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0  Step:       0  Loss:   0.912552\n",
            "Epoch:  0  Step:    1000  Loss:   0.205158\n",
            "Epoch:  0  Step:    2000  Loss:   0.479460\n",
            "Epoch:  0  Step:    3000  Loss:   0.246177\n",
            "Epoch:  0  Step:    4000  Loss:   0.098203\n",
            "Epoch:  0  Step:    5000  Loss:   0.120299\n",
            "Epoch:  0  Step:    6000  Loss:   0.082670\n",
            "Epoch:  0  Step:    7000  Loss:   0.361919\n",
            "Epoch:  0  Step:    8000  Loss:   0.099397\n",
            "Epoch:  0  Step:    9000  Loss:   0.094805\n",
            "Epoch:  0  Step:   10000  Loss:   0.096559\n",
            "Epoch:  0  Step:   11000  Loss:   0.101311\n",
            "Epoch:  0  Step:   12000  Loss:   0.090518\n",
            "Epoch:  0  Step:   13000  Loss:   0.149562\n",
            "Epoch:  0  Step:   14000  Loss:   0.093545\n",
            "Epoch:  0  Step:   15000  Loss:   0.091422\n",
            "Epoch:  0  Step:   16000  Loss:   0.095754\n",
            "Epoch:  0  Step:   17000  Loss:   0.110818\n",
            "Epoch:  0  Step:   18000  Loss:   0.098914\n",
            "Epoch:  0  Step:   19000  Loss:   0.244522\n",
            "Epoch:  0  Step:   20000  Loss:   0.096035\n",
            "Epoch:  0  Step:   21000  Loss:   0.230054\n",
            "Epoch:  0  Step:   22000  Loss:   0.106706\n",
            "Epoch:  0  Step:   23000  Loss:   0.113722\n",
            "Epoch:  0  Step:   24000  Loss:   0.101324\n",
            "Epoch:  0  Step:   25000  Loss:   0.105919\n",
            "Epoch:  0  Step:   26000  Loss:   0.097934\n",
            "Epoch:  0  Step:   27000  Loss:   0.169805\n",
            "Epoch:  0  Step:   28000  Loss:   0.093659\n",
            "Epoch:  0  Step:   29000  Loss:   0.099668\n",
            "Epoch:  0  Step:   30000  Loss:   0.115258\n",
            "Epoch:  0  Step:   31000  Loss:   0.477428\n",
            "Epoch:  0  Step:   32000  Loss:   0.131155\n",
            "Epoch:  0  Step:   33000  Loss:   0.217401\n",
            "Epoch:  0  Step:   34000  Loss:   0.106212\n",
            "Epoch:  0  Step:   35000  Loss:   0.106863\n",
            "Epoch:  0  Step:   36000  Loss:   0.332260\n",
            "Epoch:  0  Step:   37000  Loss:   0.104460\n",
            "Epoch:  0  Step:   38000  Loss:   0.104891\n",
            "Epoch:  0  Step:   39000  Loss:   0.266653\n",
            "Epoch:  0  Step:   40000  Loss:   0.114397\n",
            "Epoch:  0  Step:   41000  Loss:   0.114555\n",
            "Epoch:  0  Step:   42000  Loss:   0.105830\n",
            "Epoch:  0  Step:   43000  Loss:   0.155918\n",
            "Epoch:  0  Step:   44000  Loss:   0.112863\n",
            "Epoch:  0  Step:   45000  Loss:   0.118828\n",
            "Epoch:  0  Step:   46000  Loss:   0.104826\n",
            "Epoch:  0  Step:   47000  Loss:   0.116956\n",
            "Epoch:  0  Step:   48000  Loss:   0.105198\n",
            "Epoch:  0  Step:   49000  Loss:   0.109879\n",
            "Epoch:  0  Step:   50000  Loss:   0.113836\n",
            "Epoch:  0  Step:   51000  Loss:   0.123901\n",
            "Epoch:  0  Step:   52000  Loss:   0.111635\n",
            "Epoch:  0  Step:   53000  Loss:   0.120590\n",
            "Epoch:  0  Step:   54000  Loss:   0.131831\n",
            "Epoch:  0  Step:   55000  Loss:   0.104307\n",
            "Epoch:  0  Step:   56000  Loss:   0.105044\n",
            "Epoch:  0  Step:   57000  Loss:   0.120459\n",
            "Epoch:  0  Step:   58000  Loss:   0.122308\n",
            "Epoch:  0  Step:   59000  Loss:   0.114045\n",
            "Epoch:  1  Step:       0  Loss:   0.285074\n",
            "Epoch:  1  Step:    1000  Loss:   0.125810\n",
            "Epoch:  1  Step:    2000  Loss:   0.116220\n",
            "Epoch:  1  Step:    3000  Loss:   0.121470\n",
            "Epoch:  1  Step:    4000  Loss:   0.117989\n",
            "Epoch:  1  Step:    5000  Loss:   0.164181\n",
            "Epoch:  1  Step:    6000  Loss:   0.123958\n",
            "Epoch:  1  Step:    7000  Loss:   0.101582\n",
            "Epoch:  1  Step:    8000  Loss:   0.130444\n",
            "Epoch:  1  Step:    9000  Loss:   0.130524\n",
            "Epoch:  1  Step:   10000  Loss:   0.135231\n",
            "Epoch:  1  Step:   11000  Loss:   0.113894\n",
            "Epoch:  1  Step:   12000  Loss:   0.179876\n",
            "Epoch:  1  Step:   13000  Loss:   0.134589\n",
            "Epoch:  1  Step:   14000  Loss:   0.108447\n",
            "Epoch:  1  Step:   15000  Loss:   0.114292\n",
            "Epoch:  1  Step:   16000  Loss:   0.117601\n",
            "Epoch:  1  Step:   17000  Loss:   0.128554\n",
            "Epoch:  1  Step:   18000  Loss:   0.115676\n",
            "Epoch:  1  Step:   19000  Loss:   0.149289\n",
            "Epoch:  1  Step:   20000  Loss:   0.121895\n",
            "Epoch:  1  Step:   21000  Loss:   0.187011\n",
            "Epoch:  1  Step:   22000  Loss:   0.118793\n",
            "Epoch:  1  Step:   23000  Loss:   0.135137\n",
            "Epoch:  1  Step:   24000  Loss:   0.118069\n",
            "Epoch:  1  Step:   25000  Loss:   0.129778\n",
            "Epoch:  1  Step:   26000  Loss:   0.120525\n",
            "Epoch:  1  Step:   27000  Loss:   0.280125\n",
            "Epoch:  1  Step:   28000  Loss:   0.107371\n",
            "Epoch:  1  Step:   29000  Loss:   0.115207\n",
            "Epoch:  1  Step:   30000  Loss:   0.129446\n",
            "Epoch:  1  Step:   31000  Loss:   0.133112\n",
            "Epoch:  1  Step:   32000  Loss:   0.111714\n",
            "Epoch:  1  Step:   33000  Loss:   0.158387\n",
            "Epoch:  1  Step:   34000  Loss:   0.112591\n",
            "Epoch:  1  Step:   35000  Loss:   0.119903\n",
            "Epoch:  1  Step:   36000  Loss:   0.454918\n",
            "Epoch:  1  Step:   37000  Loss:   0.118309\n",
            "Epoch:  1  Step:   38000  Loss:   0.109851\n",
            "Epoch:  1  Step:   39000  Loss:   0.332972\n",
            "Epoch:  1  Step:   40000  Loss:   0.130319\n",
            "Epoch:  1  Step:   41000  Loss:   0.122232\n",
            "Epoch:  1  Step:   42000  Loss:   0.111994\n",
            "Epoch:  1  Step:   43000  Loss:   0.113152\n",
            "Epoch:  1  Step:   44000  Loss:   0.126266\n",
            "Epoch:  1  Step:   45000  Loss:   0.121836\n",
            "Epoch:  1  Step:   46000  Loss:   0.116775\n",
            "Epoch:  1  Step:   47000  Loss:   0.129414\n",
            "Epoch:  1  Step:   48000  Loss:   0.115219\n",
            "Epoch:  1  Step:   49000  Loss:   0.125835\n",
            "Epoch:  1  Step:   50000  Loss:   0.121134\n",
            "Epoch:  1  Step:   51000  Loss:   0.129993\n",
            "Epoch:  1  Step:   52000  Loss:   0.119065\n",
            "Epoch:  1  Step:   53000  Loss:   0.131389\n",
            "Epoch:  1  Step:   54000  Loss:   0.123803\n",
            "Epoch:  1  Step:   55000  Loss:   0.108652\n",
            "Epoch:  1  Step:   56000  Loss:   0.109061\n",
            "Epoch:  1  Step:   57000  Loss:   0.133937\n",
            "Epoch:  1  Step:   58000  Loss:   0.118248\n",
            "Epoch:  1  Step:   59000  Loss:   0.120458\n",
            "Epoch:  2  Step:       0  Loss:   0.119001\n",
            "Epoch:  2  Step:    1000  Loss:   0.132488\n",
            "Epoch:  2  Step:    2000  Loss:   0.450396\n",
            "Epoch:  2  Step:    3000  Loss:   0.121781\n",
            "Epoch:  2  Step:    4000  Loss:   0.129091\n",
            "Epoch:  2  Step:    5000  Loss:   0.124795\n",
            "Epoch:  2  Step:    6000  Loss:   0.128903\n",
            "Epoch:  2  Step:    7000  Loss:   0.111594\n",
            "Epoch:  2  Step:    8000  Loss:   0.135368\n",
            "Epoch:  2  Step:    9000  Loss:   0.133757\n",
            "Epoch:  2  Step:   10000  Loss:   0.138138\n",
            "Epoch:  2  Step:   11000  Loss:   0.118619\n",
            "Epoch:  2  Step:   12000  Loss:   0.196861\n",
            "Epoch:  2  Step:   13000  Loss:   0.141603\n",
            "Epoch:  2  Step:   14000  Loss:   0.117121\n",
            "Epoch:  2  Step:   15000  Loss:   0.126039\n",
            "Epoch:  2  Step:   16000  Loss:   0.116497\n",
            "Epoch:  2  Step:   17000  Loss:   0.159600\n",
            "Epoch:  2  Step:   18000  Loss:   0.117501\n",
            "Epoch:  2  Step:   19000  Loss:   0.142146\n",
            "Epoch:  2  Step:   20000  Loss:   0.130757\n",
            "Epoch:  2  Step:   21000  Loss:   0.178139\n",
            "Epoch:  2  Step:   22000  Loss:   0.124913\n",
            "Epoch:  2  Step:   23000  Loss:   0.148180\n",
            "Epoch:  2  Step:   24000  Loss:   0.124064\n",
            "Epoch:  2  Step:   25000  Loss:   0.120836\n",
            "Epoch:  2  Step:   26000  Loss:   0.124173\n",
            "Epoch:  2  Step:   27000  Loss:   0.122708\n",
            "Epoch:  2  Step:   28000  Loss:   0.116876\n",
            "Epoch:  2  Step:   29000  Loss:   0.120285\n",
            "Epoch:  2  Step:   30000  Loss:   0.132085\n",
            "Epoch:  2  Step:   31000  Loss:   0.109655\n",
            "Epoch:  2  Step:   32000  Loss:   0.114679\n",
            "Epoch:  2  Step:   33000  Loss:   0.395697\n",
            "Epoch:  2  Step:   34000  Loss:   0.111689\n",
            "Epoch:  2  Step:   35000  Loss:   0.123291\n",
            "Epoch:  2  Step:   36000  Loss:   0.130432\n",
            "Epoch:  2  Step:   37000  Loss:   0.121126\n",
            "Epoch:  2  Step:   38000  Loss:   0.124676\n",
            "Epoch:  2  Step:   39000  Loss:   0.113524\n",
            "Epoch:  2  Step:   40000  Loss:   0.134704\n",
            "Epoch:  2  Step:   41000  Loss:   0.121896\n",
            "Epoch:  2  Step:   42000  Loss:   0.125123\n",
            "Epoch:  2  Step:   43000  Loss:   0.486041\n",
            "Epoch:  2  Step:   44000  Loss:   0.134909\n",
            "Epoch:  2  Step:   45000  Loss:   0.128846\n",
            "Epoch:  2  Step:   46000  Loss:   0.132609\n",
            "Epoch:  2  Step:   47000  Loss:   0.134906\n",
            "Epoch:  2  Step:   48000  Loss:   0.117086\n",
            "Epoch:  2  Step:   49000  Loss:   0.127075\n",
            "Epoch:  2  Step:   50000  Loss:   0.133186\n",
            "Epoch:  2  Step:   51000  Loss:   0.129657\n",
            "Epoch:  2  Step:   52000  Loss:   0.123766\n",
            "Epoch:  2  Step:   53000  Loss:   0.128978\n",
            "Epoch:  2  Step:   54000  Loss:   0.125196\n",
            "Epoch:  2  Step:   55000  Loss:   0.105715\n",
            "Epoch:  2  Step:   56000  Loss:   0.112660\n",
            "Epoch:  2  Step:   57000  Loss:   0.141063\n",
            "Epoch:  2  Step:   58000  Loss:   0.119914\n",
            "Epoch:  2  Step:   59000  Loss:   0.130045\n",
            "Epoch:  3  Step:       0  Loss:   0.144268\n",
            "Epoch:  3  Step:    1000  Loss:   0.133289\n",
            "Epoch:  3  Step:    2000  Loss:   0.369314\n",
            "Epoch:  3  Step:    3000  Loss:   0.129857\n",
            "Epoch:  3  Step:    4000  Loss:   0.130401\n",
            "Epoch:  3  Step:    5000  Loss:   0.121130\n",
            "Epoch:  3  Step:    6000  Loss:   0.129600\n",
            "Epoch:  3  Step:    7000  Loss:   0.116140\n",
            "Epoch:  3  Step:    8000  Loss:   0.140980\n",
            "Epoch:  3  Step:    9000  Loss:   0.130412\n",
            "Epoch:  3  Step:   10000  Loss:   0.138151\n",
            "Epoch:  3  Step:   11000  Loss:   0.121694\n",
            "Epoch:  3  Step:   12000  Loss:   0.121301\n",
            "Epoch:  3  Step:   13000  Loss:   0.140527\n",
            "Epoch:  3  Step:   14000  Loss:   0.124171\n",
            "Epoch:  3  Step:   15000  Loss:   0.128098\n",
            "Epoch:  3  Step:   16000  Loss:   0.123185\n",
            "Epoch:  3  Step:   17000  Loss:   0.116793\n",
            "Epoch:  3  Step:   18000  Loss:   0.117375\n",
            "Epoch:  3  Step:   19000  Loss:   0.125557\n",
            "Epoch:  3  Step:   20000  Loss:   0.132520\n",
            "Epoch:  3  Step:   21000  Loss:   0.152694\n",
            "Epoch:  3  Step:   22000  Loss:   0.126322\n",
            "Epoch:  3  Step:   23000  Loss:   0.144573\n",
            "Epoch:  3  Step:   24000  Loss:   0.122937\n",
            "Epoch:  3  Step:   25000  Loss:   0.126205\n",
            "Epoch:  3  Step:   26000  Loss:   0.121426\n",
            "Epoch:  3  Step:   27000  Loss:   0.116124\n",
            "Epoch:  3  Step:   28000  Loss:   0.121916\n",
            "Epoch:  3  Step:   29000  Loss:   0.124153\n",
            "Epoch:  3  Step:   30000  Loss:   0.134909\n",
            "Epoch:  3  Step:   31000  Loss:   0.136403\n",
            "Epoch:  3  Step:   32000  Loss:   0.120008\n",
            "Epoch:  3  Step:   33000  Loss:   0.201090\n",
            "Epoch:  3  Step:   34000  Loss:   0.116836\n",
            "Epoch:  3  Step:   35000  Loss:   0.129502\n",
            "Epoch:  3  Step:   36000  Loss:   0.155047\n",
            "Epoch:  3  Step:   37000  Loss:   0.125644\n",
            "Epoch:  3  Step:   38000  Loss:   0.123080\n",
            "Epoch:  3  Step:   39000  Loss:   0.118427\n",
            "Epoch:  3  Step:   40000  Loss:   0.134171\n",
            "Epoch:  3  Step:   41000  Loss:   0.123832\n",
            "Epoch:  3  Step:   42000  Loss:   0.123419\n",
            "Epoch:  3  Step:   43000  Loss:   0.236280\n",
            "Epoch:  3  Step:   44000  Loss:   0.132827\n",
            "Epoch:  3  Step:   45000  Loss:   0.138340\n",
            "Epoch:  3  Step:   46000  Loss:   0.131838\n",
            "Epoch:  3  Step:   47000  Loss:   0.133358\n",
            "Epoch:  3  Step:   48000  Loss:   0.121452\n",
            "Epoch:  3  Step:   49000  Loss:   0.136095\n",
            "Epoch:  3  Step:   50000  Loss:   0.133800\n",
            "Epoch:  3  Step:   51000  Loss:   0.134193\n",
            "Epoch:  3  Step:   52000  Loss:   0.118189\n",
            "Epoch:  3  Step:   53000  Loss:   0.132867\n",
            "Epoch:  3  Step:   54000  Loss:   0.127001\n",
            "Epoch:  3  Step:   55000  Loss:   0.106668\n",
            "Epoch:  3  Step:   56000  Loss:   0.115081\n",
            "Epoch:  3  Step:   57000  Loss:   0.144772\n",
            "Epoch:  3  Step:   58000  Loss:   0.120993\n",
            "Epoch:  3  Step:   59000  Loss:   0.132733\n",
            "Epoch:  4  Step:       0  Loss:   0.139149\n",
            "Epoch:  4  Step:    1000  Loss:   0.136985\n",
            "Epoch:  4  Step:    2000  Loss:   0.414516\n",
            "Epoch:  4  Step:    3000  Loss:   0.119410\n",
            "Epoch:  4  Step:    4000  Loss:   0.131844\n",
            "Epoch:  4  Step:    5000  Loss:   0.125533\n",
            "Epoch:  4  Step:    6000  Loss:   0.130834\n",
            "Epoch:  4  Step:    7000  Loss:   0.117026\n",
            "Epoch:  4  Step:    8000  Loss:   0.145078\n",
            "Epoch:  4  Step:    9000  Loss:   0.246876\n",
            "Epoch:  4  Step:   10000  Loss:   0.143999\n",
            "Epoch:  4  Step:   11000  Loss:   0.123135\n",
            "Epoch:  4  Step:   12000  Loss:   0.117038\n",
            "Epoch:  4  Step:   13000  Loss:   0.141871\n",
            "Epoch:  4  Step:   14000  Loss:   0.125219\n",
            "Epoch:  4  Step:   15000  Loss:   0.137013\n",
            "Epoch:  4  Step:   16000  Loss:   0.124590\n",
            "Epoch:  4  Step:   17000  Loss:   0.120646\n",
            "Epoch:  4  Step:   18000  Loss:   0.115680\n",
            "Epoch:  4  Step:   19000  Loss:   0.125821\n",
            "Epoch:  4  Step:   20000  Loss:   0.141231\n",
            "Epoch:  4  Step:   21000  Loss:   0.135948\n",
            "Epoch:  4  Step:   22000  Loss:   0.129813\n",
            "Epoch:  4  Step:   23000  Loss:   0.150307\n",
            "Epoch:  4  Step:   24000  Loss:   0.126177\n",
            "Epoch:  4  Step:   25000  Loss:   0.144940\n",
            "Epoch:  4  Step:   26000  Loss:   0.122110\n",
            "Epoch:  4  Step:   27000  Loss:   0.127347\n",
            "Epoch:  4  Step:   28000  Loss:   0.123987\n",
            "Epoch:  4  Step:   29000  Loss:   0.125073\n",
            "Epoch:  4  Step:   30000  Loss:   0.138380\n",
            "Epoch:  4  Step:   31000  Loss:   0.118378\n",
            "Epoch:  4  Step:   32000  Loss:   0.125526\n",
            "Epoch:  4  Step:   33000  Loss:   0.464535\n",
            "Epoch:  4  Step:   34000  Loss:   0.122335\n",
            "Epoch:  4  Step:   35000  Loss:   0.132732\n",
            "Epoch:  4  Step:   36000  Loss:   0.169186\n",
            "Epoch:  4  Step:   37000  Loss:   0.129105\n",
            "Epoch:  4  Step:   38000  Loss:   0.122966\n",
            "Epoch:  4  Step:   39000  Loss:   0.118540\n",
            "Epoch:  4  Step:   40000  Loss:   0.142675\n",
            "Epoch:  4  Step:   41000  Loss:   0.130186\n",
            "Epoch:  4  Step:   42000  Loss:   0.123132\n",
            "Epoch:  4  Step:   43000  Loss:   0.123794\n",
            "Epoch:  4  Step:   44000  Loss:   0.135651\n",
            "Epoch:  4  Step:   45000  Loss:   0.135774\n",
            "Epoch:  4  Step:   46000  Loss:   0.133237\n",
            "Epoch:  4  Step:   47000  Loss:   0.132102\n",
            "Epoch:  4  Step:   48000  Loss:   0.121484\n",
            "Epoch:  4  Step:   49000  Loss:   0.145714\n",
            "Epoch:  4  Step:   50000  Loss:   0.137311\n",
            "Epoch:  4  Step:   51000  Loss:   0.132715\n",
            "Epoch:  4  Step:   52000  Loss:   0.116403\n",
            "Epoch:  4  Step:   53000  Loss:   0.130311\n",
            "Epoch:  4  Step:   54000  Loss:   0.128680\n",
            "Epoch:  4  Step:   55000  Loss:   0.113708\n",
            "Epoch:  4  Step:   56000  Loss:   0.115551\n",
            "Epoch:  4  Step:   57000  Loss:   0.141975\n",
            "Epoch:  4  Step:   58000  Loss:   0.131297\n",
            "Epoch:  4  Step:   59000  Loss:   0.135810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNDmPl9Vs84H",
        "outputId": "24618238-59dc-4c33-fde3-a2e3194eb672"
      },
      "source": [
        "# Test\n",
        "\n",
        "test_data_list = np.loadtxt('./mnist_test.csv', delimiter=',', dtype=np.float32)\n",
        "\n",
        "input_data = test_data_list[:, 1:]\n",
        "target_data = test_data_list[:, 0]\n",
        "\n",
        "model.predict(input_data, target_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.9623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDxE7ITTz2qg"
      },
      "source": [
        "# Accuracy 비교\n",
        "\n",
        "# 가중치 초기화 방법 다르게\n",
        "# Xavier: 0.9654\n",
        "# random: 0.8543\n",
        "# zero: 0.9394\n",
        "\n",
        "# bias 업데이트 식 다르게\n",
        "# loss_3 = (h-y)*y*(1-y) : 0.9672\n",
        "# loss_3 = h-y : 0.8582"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}