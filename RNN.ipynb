{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_myself.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1nk-tAJ_OZ4Ydi0zC10BBgbaylDdFkjHc",
      "authorship_tag": "ABX9TyP9cb4I/DcU5GWUmjmr72p8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK4ITyXbTZ_Y"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17FeEr3kTi96",
        "outputId": "84e141d1-b80c-41fa-a6af-85912e94a61d"
      },
      "source": [
        "cd drive/MyDrive/머신러닝 실습/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/머신러닝 실습\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAAxc8mNTm9c"
      },
      "source": [
        "data = open('text.txt', 'r').read()\n",
        "vocabs = list(set(data))\n",
        "data_size = len(data)\n",
        "vocab_size = len(vocabs)\n",
        "\n",
        "print(data_size, vocab_size)\n",
        "\n",
        "ch_to_idx = {ch: idx for idx, ch in enumerate(vocabs)}\n",
        "idx_to_ch = {idx: ch for idx, ch in enumerate(vocabs)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6TK2B-UUK-y"
      },
      "source": [
        "# hyperparameter\n",
        "hidden_size = 100\n",
        "seq_length = 25\n",
        "learning_rate = 0.1\n",
        "\n",
        "# model parameter\n",
        "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01\n",
        "Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "Why = np.random.randn(vocab_size, hidden_size) * 0.01\n",
        "bh = np.zeros([hidden_size, 1])\n",
        "by = np.zeros([vocab_size, 1])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laXuQWR0VYCZ"
      },
      "source": [
        "def softmax(x):\n",
        "  return np.exp(x) / np.sum(np.exp(x))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyqVRgfNVdX4"
      },
      "source": [
        "def loss_func(inputs, targets, hprev):\n",
        "  \n",
        "  # forward\n",
        "  xs, hs, ys, ps = {}, {}, {}, {}\n",
        "  hs[-1] = hprev.copy()\n",
        "  loss = 0\n",
        "\n",
        "  for t in range(len(inputs)):\n",
        "    xs[t] = np.zeros([vocab_size, 1])\n",
        "    xs[t][inputs[t]] = 1\n",
        "\n",
        "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + bh + np.dot(Whh, hs[t-1]))\n",
        "    ys[t] = np.dot(Why, hs[t]) + by\n",
        "    ps[t] = softmax(ys[t])\n",
        "\n",
        "    hypothesis = ps[t][targets[t]][0]\n",
        "    loss += -np.log(hypothesis)\n",
        "\n",
        "  \n",
        "  # backward\n",
        "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
        "  dhnext = np.zeros_like(hs[0])\n",
        "\n",
        "  for t in reversed(range(len(inputs))):\n",
        "    dy = ps[t].copy()\n",
        "    dy[targets[t]] -= 1\n",
        "\n",
        "    dby += dy\n",
        "    dWhy += np.dot(dy, hs[t].T)\n",
        "    dh = np.dot(Why.T, dy) + dhnext\n",
        "\n",
        "    dhraw = (1 - hs[t] * hs[t]) * dh\n",
        "\n",
        "    dbh += dhraw\n",
        "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
        "    dhnext = np.dot(Whh.T, dhraw)\n",
        "    dWxh += np.dot(dhraw, xs[t].T)\n",
        "\n",
        "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "    np.clip(dparam, -5, 5, out=dparam)\n",
        "\n",
        "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw14OT-_ZeHn"
      },
      "source": [
        "def sample(seed_idx, n, h):\n",
        "  x = np.zeros([vocab_size, 1])\n",
        "  x[seed_idx] = 1\n",
        "\n",
        "  idxes = []\n",
        "  for t in range(n):\n",
        "    h = np.tanh(np.dot(Wxh, x) + bh + np.dot(Whh, h))\n",
        "    y = np.dot(Why, h) + by\n",
        "    p = softmax(y)\n",
        "\n",
        "    idx = np.random.choice(range(vocab_size), p=p.ravel())\n",
        "    x = np.zeros([vocab_size, 1])\n",
        "    x[idx] = 1\n",
        "    idxes.append(idx)\n",
        "\n",
        "  return idxes"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGRvoIGqa6as"
      },
      "source": [
        "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "mbh, mby = np.zeros_like(bh), np.zeros_like(by)\n",
        "smooth_loss = -np.log(1.0 / vocab_size) * seq_length"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc6Cg9gieWYY"
      },
      "source": [
        "iter = 0\n",
        "data_ptr = 0\n",
        "\n",
        "while True:\n",
        "  if iter == 0 or data_ptr+seq_length+1 >= data_size:\n",
        "    hprev = np.zeros([hidden_size, 1])\n",
        "    data_ptr = 0\n",
        "\n",
        "  inputs = [ch_to_idx[ch] for ch in data[data_ptr:data_ptr+seq_length]]\n",
        "  targets = [ch_to_idx[ch] for ch in data[data_ptr+1:data_ptr+seq_length+1]]\n",
        "\n",
        "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = loss_func(inputs, targets, hprev)\n",
        "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "  if iter % 500 == 0:\n",
        "    print('iter: ', iter, 'loss: ', smooth_loss)\n",
        "\n",
        "  # sample\n",
        "  if iter % 500 == 0:\n",
        "    sample_idxes = sample(inputs[0], 200, hprev)\n",
        "    text = ''.join(idx_to_ch[idx] for idx in sample_idxes)\n",
        "    print('==========\\n', text, '===========\\n')\n",
        "\n",
        "  # 파라미터 update (Adagrad)\n",
        "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], [dWxh, dWhh, dWhy, dbh, dby], [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "    mem += dparam * dparam\n",
        "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
        "\n",
        "  iter += 1\n",
        "  data_ptr += seq_length"
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}